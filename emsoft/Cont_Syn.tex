%!TEX root = main.tex
\section{Controller Synthesis}\label{sec:Controller_Synthesis}

\begin{figure}
\begin{lstlisting}[language=Python,numbers=left,numbersep=3pt,frame=lines,keepspaces=true,mathescape=true,basicstyle=\small\ttfamily,escapeinside={<@}{@>}]
 model = input() 
 $\Delta$ = input()
 $\varepsilon_{Q}$ = input()
 assert ($\Delta$ >= 0 and $\varepsilon_{Q}$ > 0)
$\varepsilon$ = $\max_i \{||F_i||_2\}\varepsilon_{A/D}+\varepsilon_{Q}$$\quad$//width of the tube

 while true:
   $F$, $G$, $H$, $K$ = design_robust_MPC(model, $\Delta$)
   errorRegion = incorrect_region_error($F$, $G$, $H$, $K$, $\varepsilon$)
   if $\Delta$ > errorRegion:
     errorAct = $\Delta$ - errorRegion
     $\hat{F}$, $\hat{G}$ = precision_tuning($F$, $G$, errorAct)
     $\hat{H}$, $\hat{K}$ = precision_tuning($H$, $K$, $\varepsilon_Q$)
     return $\hat{F}$, $\hat{G}$, $\hat{H}$, $\hat{K}$
   else:
     $\Delta$ = errorRegion + $\varepsilon_{SAFE}$
\end{lstlisting}
\caption{Algorithm for designing memory-efficient robust EMPC controller}
\label{lst:alg}
\end{figure}


%\eva{The algorithm in \autoref{lst:alg} needs to be fixed:
%\begin{itemize}
%\item Use the math symbol for Delta to be consistent with the rest of the paper
%\item the function design\_robust\_MPC seems to do a lot more than just designing the controller, i.e. it seems to return an error bound instead of a controller. Split into more steps.
%\item \maxUij and $err_{max}(u_{i})_{p}$ are not good variable names, use a proper variable name
%$\item UNI\_MIX\_precision should also be renamed to something more helpful, e.g. tune\_precision?
%\end{itemize}
%}

In the previous section, we assumed a given precision $p$. In this section we
present our algorithm for deriving suitable values of $p$ fully automatically.
Further, while~\autoref{sec:Error_Analysis} assumed a uniform finite precision,
our tuning algorithm derives possibly different precisions for different control
values.

%\subsection{Algorithm}
\autoref{lst:alg} shows a high-level view of our optimization algorithm for
implementing robust MPC controllers with guaranteed error bounds.
Given a state-space model of a plant (line 1), our algorithm returns four matrices
$\hat{F}$, $\hat{G}$, $\hat{H}$, and $\hat{K}$,
which soundly implement a robust explicit MPC controller in mixed-precision
fixed-point arithmetic.
Our algorithm takes two additional inputs, $\Delta$ and $\varepsilon_Q$ (line 2, 3). 
The disturbance $\Delta$ provided by the user represents a starting point for 
the search for a robust controller. In principle, the user can set $\Delta = 0$,
however note that the finite-precision implementation of the controller will always
incur at least some small error. In practice we thus start the search with a slightly
larger value, e.g. $\Delta = 0.1$. Our algorithm will perform a linear search and
increase $\Delta$ if needed.

The third input parameter is $\varepsilon_Q$. In~\autoref{sec:Error_Analysis} we
showed that if the user provides a precision $p$, then we can automatically
compute $\varepsilon_Q$.
Now, we want to \emph{derive} a suitable precision $p$, however, and face an
issue. In order to derive $p$ for some expression, Daisy and any other precision
tuning tool requires an error bound which should be satisfied. That is, either a
precision $p$ is given, and we can compute an error bound, or the error bound is
given and we can derive a precision $p$. We cannot do both at the same time; the
problem would be underconstrained. We solve this chicken-and-egg problem by
requiring the user to provide $\varepsilon_Q$ as part of the input.
%\Mahmoud{
%	\begin{remark}\label{rem:eps_Q}
%		Typical values for $\varepsilon_Q$ can range from $10^{-4}$ up-to $10^{-2}$. Increasing the $\varepsilon_Q$ leads into more memory save over $H$ and $K$, while requiring more precision for $F$ and $G$ (\autoref{tab:di}). Of course, too large values for $\varepsilon_Q$ can result in failure of RMPC design. However, this is usually not the case over reasonable choices for $\varepsilon_Q$.
%	\end{remark}}

We need to distribute the available ``disturbance budget'' among the two error
sources: error due to selecting the wrong region, which is given by the
quantization of $\hat{H}$ and $\hat{K}$, and the error due to
quantization of the control action, given by $\hat{F}$, $\hat{G}$.
The quantization of $\hat{H}$ and $\hat{K}$ is determined by
$\varepsilon_Q$, thus by providing this input value, the user effectively
chooses how much precision to allow for $\hat{H}$ and $\hat{K}$.
Note that this precision comes at the expense of the precision of
$\hat{F}$ and $\hat{G}$, i.e. the more accurately $\hat{H}$
and $\hat{K}$ are represented, the more approximate $\hat{F}$ and
$\hat{G}$ need to be to fit into the disturbance budget, and vice versa.
We note that one could straight-forwardly extend our algorithm to include an
outer loop which would explore different values for $\varepsilon_Q$. For our
experiments in~\autoref{sec:experiments}, we vary this value manually.


% From line 1 to 3, the procedure waits for input parameters from the designer: $\delta$ and $\varepsilon_{Q}$. $\delta$ is the disturbance . It is a problem dependent parameter, and thus can be any positive number. Typically, it is in the order of 0.1.
% $\varepsilon_{Q}$ is the imprecision error-bound for the quantization of borders. 
% The latter has to be strictly greater than zero, otherwise we could rely on infinite precision for storage of borders. Usually, the value for $\varepsilon_{Q}$ is in the order of 0.001.

Unlike the initial $\Delta$, the parameter $\varepsilon_Q$ needs to be strictly positive (line 4)
for mixed-precision tuning tools to work correctly. 
Typical values for $\varepsilon_Q$ can range from $10^{-4}$ up to $10^{-2}$. 
%\RM{repetition:}
%Increasing $\varepsilon_Q$ leads to more memory savings over $H$ and $K$ while requiring more precision for $F$ and $G$. 
%\RM{end}
%Of course, a large value for $\varepsilon_Q$ can result in failure of the RMPC design. 
%\RM{imprecise:}However, this is usually not the case over reasonable choices for $\varepsilon_Q$.
%\RM{omit?}
Line 5 computes the width of the tubes $\varepsilon$ as shown in~\autoref{eq:epsilontot},
which will be used to bound the error due to selecting the wrong region
($\varepsilon_{A/D}$ is a constant parameter dependent on the specifics of the
converter).

The algorithm then calls Matlab's multi-parametric toolbox to design an explicit
MPC controller for the given state-space model which is robust to the initial
disturbance $\Delta$ (line 8). The result is a controller represented by four matrices
$F$, $G$, $H$, $K$ given in high precision.

\emph{Assuming} that we can implement the polyhedral region partitioning (given by
matrices $H$, $K$) with an implementation error of at most $\varepsilon_Q$,
our algorithm proceeds to compute the error due to selecting the wrong region (line 9).
This computation works as explained in~\autoref{sec:Error_Analysis}, \autoref{eq:maximization}.

% Once input parameters are verified, in line 7 we design the controller in MATLAB
% multi-parametric toolbox. The suite receives in input the coefficient $\delta$,
% and the state space model for the plant (e.g. inverted pendulum or aircraft) we
% need to control. The suite returns a piecewise affine function (F and G)
% together with a polytopes region partitioning (H and K).

% In line 8, we compute the incorrect region error: we build the tube regions around the borders (\ref{eq:tube})  using $\varepsilon$ and we compute (\ref{eq:maximization}) for the corner points of the tube.

The algorithm then checks whether the region error remains below the disturbance
bound used for the synthesis of the controller (line 10). 
If the error already exceeds $\Delta$, we do not attempt to compute a finite-precision
assignment, as the controller design is already infeasible.
In this case, we increase $\Delta$ by a fixed (small) amount $\varepsilon_{SAFE}$ (line 16) and 
design a new controller. 

% In the conditional in line 9, we check whether the robustness $\delta$ is greater than the incorrect region error. If this is not the case, the initial value for $\delta$ was not robust to the magnitude of this error, and we need to re-design the controller with a greater $\delta$. 
% Indeed in line 15, the value $\delta$ for the next iteration has to be at least the same magnitude of the current errorChoise. Moreover, we increase by a small scalar $\varepsilon_{SAFE}$ (in the order of 0.001) the value for $\delta$ to speed-up the convergence of the analysis. 

If the region error is smaller than $\Delta$, we still have an error budget \texttt{errorAct} left
over to implement the control actions in finite precision (line 11).
For the precision assignment, we first call Daisy with the expressions for the
control actions given by $F$ and $G$, and the remaining error
budget as the error bound (line 12).
Then we call Daisy with the expressions of the region bounds given by $H$
and $K$, now with the error bound $\varepsilon_Q$ (line 13).

In both cases, Daisy determines a quantization of the input matrices (Daisy also
determines the fixed-point formats of the arithmetic operations, which we ignore here).
Daisy either computes a minimal uniform fixed-point precision (i.e. word length),
or it returns a mixed-precision assignment, where each value and
intermediate operation can potentially have a different word length.
To determine the minimal uniform precision, Daisy performs a linear search.
It starts from the smallest precision (1 bit) and checks whether it satisfies the
error bound. If not, the precision is increased by 1 bit and the first precision
that satisfies the error bound is returned.
Minimal uniform precision is used as a starting point for mixed-precision tuning,
which attempts to assign a lower precision to some variables. 
In our experiments in~\autoref{sec:experiments}, we compare the two modes and show
that mixed precision leads to smaller memory footprints.

Note that Daisy always returns a quantization (at least for our linear
expressions). If the error bound given is very small (but larger than zero),
Daisy will return a fixed-point precision with a large number bits, but will not
fail.

\smallskip
\noindent
\textbf{Termination of the algorithm.}
From a theoretical point of view, the error due to a wrong region selection does not necessarily converge since each new designed controller may differ from the previous ones. Therefore, it is not easy to give an upper bound over the number of times that the robust MPC design needs to be repeated. However, by rewriting \autoref{eq:RMPC_prob}, we have
\begin{align*}
\nu_0(x_0, \Delta),\ldots&,\nu_{N-1}(x_0,\Delta)=\\
\mathrm{argmin} & \max_{w_0,\cdots,w_{N-1}} \sum_{i=0}^{N-1}(x_i^TQx_i+u_i^TRu_i) + x_N^TQ_Fx_N\nonumber\\
\quad  \text{s.t.} \quad & x_{i+1}=Ax_i+Bu_i + E w_i, \quad\forall i\in\{0,1,\cdots,N-1\}\nonumber\\
\quad \quad & u_i=\mu x_i+v_i, \quad\forall i\in\{0,1,\cdots,N-1\}\nonumber\\
\quad \quad & u_i\in\mathcal{U},x_i\in\mathcal{X},\quad \forall w_i\in\mathcal{W}_{\Delta},\,\,\forall i\in\{0,1,\cdots,N\}.
%&x_i\in\mathcal{X},\quad \forall w_i\in\mathcal{W},\quad\forall i\in\{0,1,\cdots,N\},
\end{align*}
Assuming the optimization has a unique solution for every $\Delta$, 
one may use the results on continuity of the $\mathrm{argmin}$ function to compute an upper bound on the 
Lipschitz constant of the optimal input $\nu = \kappa(x)$ in Equation~\eqref{eq:affine_map} that holds for all $\Delta$. 
The computed bound will only depend on the dynamics of the system depicted in \autoref{eq:DSS} and hence can be used 
to evaluate the maximum error over incorrect region selection. 
This bound can be quite large, leading to a very conservative bound on the maximum number of iterations before convergence. 
From an experimental point of view, our observations show that for reasonable range of parameters, convergence was
always reached after only a few iterations.

\begin{comment}
\RM{I took out the following section because it was too hacky. The complexity should be given in terms of the original
problem input, but we keep inventing new parameters. Also, $n$ below in the complexity of mixed precision tuning doesn't make
sense. In case we bring this back, it should be fixed.}
\smallskip
\noindent
\textbf{Computational complexity.} 
It is shown in \cite{Alessio2009} that the number of regions $P$ 
in Equation~\eqref{eq:affine_map} is upper bounded by $2^q$, where $q$ is the total number of constraints in \autoref{eq:RMPC_prob}.
Assuming that each region has at most $N$ hyperplanes, computation of the incorrect region selection error requires at 
most $i\times N\times 2^q$ affine computations, where $i$ denotes the number of times the RMPC design updates $\Delta$ before convergence. 
In our experiments, the computation of the incorrect region selection error takes only a few minutes for our largest tested benchmarks. 
The complexity of the precision tuning is linear with respect to the number of regions and hyperplanes. 
So, $(P+ N\times P)$ is an upper bound for the number of problems we need to encode in Daisy.
In each encoding, the uniform-precision detection %takes linear number of steps \RM{in what parameter?} before convergence. 
%It 
starts from 1-bit and increases the precision one by one until the first valid configuration. 
The complexity of mixed-precision tuning with delta-debugging is $n\times log(n)$ in average, 
and bounded by $n^2$ in the worst case, where $n$ is the number of variables. \RM{how is $n$ related to the other parameters?}

\end{comment}


\subsection{Implementation}

We implemented our algorithm in a Python script which interfaces between Matlab
and Daisy and which implements the high-level structure from~\autoref{lst:alg}.
The interface parses the output from Matlab and encodes the matrices $F$,
$G$, $H$, $K$ and the error bounds in Daisy's input format. 

We set up the problem with YALMIP~\cite{matlabYALMIP}, and use 
Matlab's MPT toolbox~\cite{matlabMPT} to compute the robust explicit MPC.

We run the precision tuning for control actions (line 12) and hyperplanes (line 13) in
parallel, because the analysis of a single region or hyperplane is completely independent from
the others. 
We currently first perform precision tuning for control actions and then for
hyperplanes. These two steps could be run concurrently as well, though the
impact is likely going to be minimal because the number of hyperplanes is
usually an order of magnitude greater than the number of regions.

% A relatively simple improvement to the current parallelization could be to run precision tuning for hyperplanes and active functions together, without waiting for the latter to terminate. Nevertheless, we suspect a minimal impact on performance, because usually the number of hyperplanes is an order of magnitude greater than regions. Thus, this is the main bottleneck of the algorithm. 
%\eva{Can we say something about parallelization?}

% We wrote a script Python to interface the design of the controller in MATLAB Toolbox with the precision tuning phase in Daisy. The script divides in two main components: (i) a parser to process the output from MATLAB suite and (ii) a procedure to encode the vectors F,G and H,K in expressions formatted for Daisy.
