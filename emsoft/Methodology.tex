\section{Methodology}
In this work we present an innovative method for the off-line design (explicit) of robust MPC controllers for linear time invariant systems.

Consider the (discrete) state space model:
\begin{flalign}\label{eq:statespacemodel}
& x_{k+1} = A\statevar + Bu_{k}
%& y_{k} = C\statevar\nonumber
\end{flalign}

where $\statevar\in\mathbb{R}_{n}$ is the state variable at time $k$, $u_{k}$ is the scalar control input, and $A\in\mathbb{R}^{nxn}$ and $B\in\mathbb{R}^{n}$ guarantee the stability of the system at runtime.

The controller $u_{k}$ for (\ref{eq:statespacemodel}) is found by solving an off-line optimization problem. We rely on MATLAB multi-parametric toolbox to solve such optimization problem~\cite{matlabMPT, matlabYALMIP}. 
The output of the suite is a continuous piecewise affine (PWA) function, together with a partitioning of the domain of \statevarmath. Each element in the partitioning is called region.
A generic region $i$ consists in a n-polytope, uniquely identified by a set of inequalities in the form: $H_{i}\statevar\le K_{i}$. 

We call \statespace\space the union of all regions $X_{i}$ in the domain of the state variable $x_{k}$:
\begin{equation}
\statespace = \bigcup_{i}(H_{i}\statevar\le K_{i})
\end{equation}

%https://en.wikipedia.org/wiki/Polyhedron
%https://en.wikipedia.org/wiki/Polytope

Each region $i$ is associated with an activation function $u_{i,k}=F_{i}x_{k}+G_{i}$. At runtime, the value of state variable \statevarmath  is compared against the polytopes bounds: depending on the region $i$ containing \statevarmath, the corresponding activation function $u_{i}$ is computed.


A drawback in the design of explicit MPC is that both regions boundaries ($H$ and $K$) and activation functions ($F$ and $G$), have to be stored on the micro-controller. Usually these devices have limited memory, in the order of KBs.



This work does not rely on any specific technique to verify which region \statevarmath belongs to, so we consider a linear search over all the regions in \statespace\space, using a case-of conditional statement similar to the one in Listing \ref{lst:caseof}.

\begin{lstlisting}[escapeinside={(*}{*)},label={lst:caseof}, caption=switch for region selection]
switch((*\statevarmath*)):
case (*$ H_{1}\statevar<=K_{1}$*) then (*$u_{1}$*)
case (*$ H_{2}\statevar<=K_{2}$*) then (*$u_{2}$*)
case (*$ H_{3}\statevar<=K_{3}$*) then (*$u_{3}$*)
...
case (*$ H_{n}\statevar<=K_{n}$*) then (*$u_{n}$*)
\end{lstlisting}

At design time, the designer specifies the maximal disturbance value the controller can tolerate: we call this numerical value \texttt{delta}. 

\texttt{delta} has to be an upper bound for both errors generated by the system itself (e.g. noise from sensors) and disturbance coming from sources external to the system (e.g. friction). The controller is then designed to be robust against any kind of disturbance up to a maximal value \texttt{delta}.

Among the sources of disturbance targeting the output of the controller, our goal is to assure that the approximation error caused by the use of finite precision arithmetic is also bounded by this design property.

In particular, (i) first we want to guarantee that the finite precision implementation of the controller produces an approximation error that is at most equal to delta.

(ii) Second, we want to take advantage of this disturbance delta, and reduce the arithmetic precision used for computations and storage, while still guarantee that the approximation error is bounded by delta. Our intuition is that a greater value for delta allows for a reduced precision for computations (e.g. with respect to 32bits precision).

\subsection{Error Analysis}
Since (i) any measurement of the plant comes with some uncertainty (e.g. uncertainty from sensors) but also (ii) due to analog-digital conversion, the value of state variable \statevarmath comes with some numerical errors. The equation for \statevarmath is than defined as:
\begin{equation}
\qstatevar=\statevar + \texttt{err}
\end{equation}
where \statevarmath is the true (unknown) measure of the plant, while \qstatevarmath is the actual value.

\texttt{err} generates instability when it is time to verify which region \qstatevarmath belongs to: it can happen that \statevarmath satisfies the bounds of region $i$, but because of \texttt{err}, \qstatevarmath belongs to region $j$. This scenario becomes more intuitive when \qstatevarmath falls \texttt{close} to the border between two neighbor regions $i$ and $j$, but in general this instability depends on the magnitude of error \texttt{err}.

Assuming that all points in region $i$ might be erroneously assigned to region $j$, without any  knowledge of the actual value of error \texttt{err}, is a too wide over-approximation of the existing instability. 

Our goal is to build a feasible geometrical space around the border between two neighbor regions $i$ and $j$, where actually it is feasible that \statevarmath belongs to region $i$ but \qstatevarmath follows in region $j$ (or vice-versa).
We call this geometrical space the \texttt{tube}.

There are two main sources of error affecting the size of the \texttt{tube}: (i) the first one is caused by analog-digital conversion, happening just before the controller receives an estimation of the plant from the sensors. We call this error $\varepsilon_{A/D}$: 

\begin{equation}\nonumber
\varepsilon_{A/D}=\frac{V_{cc}}{2^{p}-1}
\end{equation}

Where $V_{cc}$ is the reference voltage of the converter (e.g. typical 5V) and $p$ represents the number of bit of the processor.

The second error affecting the size of the \texttt{tube} is caused by (ii) the quantization of region bounds in the memory of the micro-controller. We call this error $\varepsilon_{Q}$ for error quantization.

While $\varepsilon_{A/D}$ is intrinsic in the capabilities of the device, $\varepsilon_{Q}$ depends on the precision used to store the boundaries. This second error can be regulated based on a trade-off among accuracy of the storage and memory save~\cite{memoryMPC}.

The total size of the \texttt{tube} is then:
\begin{equation}\label{eq:epsilontot}
\varepsilon=\varepsilon_{A/D}+\varepsilon_{Q}
\end{equation}
\subsection{Finite Precision Implementation}
The output of the controller can be affected by two main errors: (i) the controller chooses the wrong activation function because of $\varepsilon$ in (\ref{eq:epsilontot}), and (ii) the approximation error deriving from finite precision arithmetic used to compute the activation function itself~\cite{imperialrmpc}.

The effect of picking the wrong activation function are similar to instability in the switch reported in Listing \ref{lst:caseof}. 

In an hypothetic scenario where all the measurements were done in infinite precision arithmetic and without any uncertainty, we assume the controller $i$ would be activated. Instead, because we cannot rely on infinite precision, controller $j$ is selected. 

The error committed is the difference between the output of the two branches $i$ and $j$.

The system has to be robust against a mistake in choosing controller $i$ instead of $j$ (or vice-versa), only when \qstatevarmath falls into the \texttt{tube} between $i$ and $j$, and not for all the points in the two regions. Moveover, we are interested in the worst case scenario where is maximized the difference between the activation functions of any two neighbor regions.

In the following, we assume $i$ and $j$ are the index of two generic regions in \statespace:
\begin{flalign}
\label{eq:maximization}
&\max_{\forall i,j\;|\;neighbour(i,j)}|u_{i}-u_{j}| = \\
&\max_{\forall i,j\;|\;neighbour(i,j)}|F_{i}\statevar+G_{i} - (F_{j}\statevar+G_{j})|\nonumber
\end{flalign}
where \statevarmath belongs to the \texttt{tube} between region $i$ and $j$.
Because of the linearity of the function $u_{i}-u_{j}$, and because of the convexity of the regions $i$ and $j$, it is enough to evaluate function (\ref{eq:maximization}) at the corner points of the \texttt{tube}, instead of solving a maximization problem.

%The geometrical space where $\hat{X}$ activates $U_{i}$, while X would activate $U_{j}$ can be bounded thanks to a preliminary knowledge of the error $\varepsilon$: the distance between region $i$ and $j$ such that:
%\begin{equation}
%\hat{X}-X <= \varepsilon
%\end{equation}
%and $\hat{X}$ belongs to region $i$ but $X$ to $j$ (or vice-versa).

We compute (\ref{eq:maximization}) for all pairs of neighbor regions $i$ and $j$. Two regions are neighbors when they share at least a border.

In formula $\exists\; m,n \;$such that:
\begin{equation}
(H_{i}\statevar-K_{j})_{m} = (H_{j}\statevar-K_{j})_{n}
\end{equation}
where $m$ and $n$ are the index of the two matching borders. We label with $border_{i,j}$ the matching border shared among the two regions.

Starting from the equation of $border_{i,j}$, we delimit the geometrical space where it makes sense to compute (\ref{eq:maximization}) with: 
\begin{equation}
\begin{aligned}
(border_{i,j} >= -\varepsilon) \land
(border_{i,j} <= \varepsilon)
\end{aligned} 
\end{equation}
where $\varepsilon$ is defined in (\ref{eq:epsilontot}). We call such geometrical space the \texttt{tube} between $i$ and $j$.

When \qstatevarmath belongs to the \texttt{tube}, it might be that $u_{i}$ is activated instead of $u_{j}$ (or vice-versa). Otherwise, when \qstatevarmath does not belong to the tube, no matter the error $\varepsilon$, the right activation function is activated, and we consider only the error deriving from the computation itself.

Since computing $u_{i}$ introduces approximation error because of finite precision arithmetic, the system has to be robust against an error that is:

\begin{equation}\label{eq:fperror}
err(u_{i})_{p}=|(\hat{F}_{p}-F)\qstatevar+(\hat{G}_{p}-G)|
\end{equation}

where $\hat{F}$ and $\hat{G}$ represent the rounded values (in p bits) for the infinite precision values $F$ and $G$.

In (7) the domain of \qstatevarmath are all the points in region $i$, but also all the values in the \texttt{tube} between region $i$ and any of the neighbors of $i$. Even if some points in the tube do not belong to region $i$, it might that $u_{i}$ is (erroneously) activated.

We compute (\ref{eq:fperror}) for all regions in \statespace\space and we take the maximum value of the error (worst case):

\begin{equation}\label{eq:maxfperror}
\max_{\forall \regionimath{i}\;in\;\statespace} err(u_{i})_{p}
\end{equation}


The disturbance \texttt{delta}  has to be an upper bound to the summation of (\ref{eq:maximization}) and (\ref{eq:maxfperror}).

In formula:
\begin{flalign}
\label{eq:delta}
&delta >= \\
&\max_{\forall i,j\;|\;neighbour(i,j)}|u_{i}-u_{j}| + \max_{\forall\;\regionimath{i}\;in\;\statespace} err(u_{i})_{p}\nonumber
\end{flalign}

\subsection{Precision Tuning}
In formula (\ref{eq:epsilontot}), the equation for $\varepsilon$ contains: $\varepsilon_{A/D}$ that is the error introduced by the analog-digital conversion, and $\varepsilon_{Q}$ that is generated by the quantization of hyperplanes $H\statevar\le K$ in a finite number of bits.
This $\varepsilon_{Q}$ can be tuned based on memory availability in the micro-controller.
The rule for $\varepsilon_{Q}$ is the following:
\begin{equation}\label{eq:quantizationlines}
\varepsilon_{Q} > (H-\hat{H}_{p})\qstatevar+(K-\hat{K}_{p})
\end{equation}
This inequality assures that the distance between any hyperplane represented in infinite precision ($H$ and $K$), and its counter-part quantized in p bits ($(\hat{H})_{p}$ and $(\hat{K})_{p}$), is bounded by $\varepsilon_{Q}$. 

Our goal is to solve (\ref{eq:quantizationlines}) with respect to p: assign an arbitrary value to $\varepsilon_{Q}$ and find the minimal-precision p such that the inequality holds.

The main advantage in this approach (compared to fixing the precision p a priori), is that the value of p can be tuned based on the memory availability in the micro-controller.

To solve (\ref{eq:quantizationlines}) we used Daisy: a static analyzer for finite precision expressions, ables to provide a sound upper-bound to the maximal approximation error of a formula with respect to its real (infinite precision) counterpart. Since the magnitude of the round-off error depends on the range of input variables, any variable encoded in Daisy has to be bounded. 

Since the state variable \statevarmath is bounded by \statespace, and $F,G,H$ and $K$ are vectors of constants, we can rely on Daisy for this verification step.

In a similar way, we use Daisy to solve the inequality in (\ref{eq:delta}) with respect to p:
\begin{flalign}
\label{eq:deltaminusmax}
&delta - \Big(\max_{\forall i,j\;|\;neighbour(i,j)}|u_{i}-u_{j}|\Big)>=\\
& \max_{\forall\;\regionimath{i}\;in\;\statespace} err(u_{i})_{p}\nonumber
\end{flalign}

Be aware that the left side of the inequality is not parametrized in p. Only the right size of (\ref{eq:deltaminusmax}) actually depends on the precision p. After we come up with a numerical value for the left side of the inequality, we encode (\ref{eq:deltaminusmax}) in Daisy in the same way done for (\ref{eq:quantizationlines}).

\subsection{Algorithm}

\begin{lstlisting}[language=Python,numbers=left,numbersep=3pt,frame=lines,keepspaces=true,escapeinside={(*}{*)},caption=design of robust MPC with verification and precision tuning,label={lst:alg}]
delta=input()
(*$\varepsilon_{Q}$*)=input()
assert (delta>=0 and (*$\varepsilon_{Q}$*)>0)
(*$\varepsilon$*)=(*$\varepsilon_{A/D}$*)+(*$\varepsilon_{Q}$*)

while True:
design_robust_MPC(delta)
maxUij = compute (*(\ref{eq:maximization})*) with size(tube)=(*$\varepsilon$*)
if delta > maxUij:
max_err=delta-maxUij
UNI_MIX_precision($F$,$G$,max_err)
UNI_MIX_precision($H$,$K$,(*$\varepsilon_{Q}$*))
break
else:
delta=maxUij+(*$\varepsilon_{SAFE}$*)
\end{lstlisting}
In Listing \ref{lst:alg} we describe the procedure used to design a robust MPC controller such that (\ref{eq:delta}) is respected.

First, the designer fixes the initial values for delta and $\varepsilon_{Q}$.

Even if it possible to assign value zero to delta~\cite{imperialrmpc}, we do not encourage such initialization value: it is going to fail the analysis (at least) for the first iteration because the error in (\ref{eq:maxfperror}) is going to be greater than zero (in the remote scenario where  (\ref{eq:maximization}) is equal to zero). A better initialization would be to set delta to an arbitrary small value, slightly greater than zero.

On the other hand, the value for $\varepsilon_{Q}$ has to be strictly greater than zero, otherwise we could rely on infinite precision for p in (\ref{eq:quantizationlines}).

Once input parameters are verified, we use MATLAB toolbox to design a controller with robustness value equal to delta. 

Then, we compute (\ref{eq:maximization}) and we compare the result with delta: we are aware that the computation of (\ref{eq:maximization}) is not exact (even if it is done in 64bits precision), but usually the computation of (\ref{eq:maximization}) results in a value that is several orders of magnitude greater than the error of the computation itself. For this reason we consider this approximation error negligible from the point of view of our analysis. 

Again, in the conditional inside the loop, delta has to be strictly greater than \texttt{maxUij} otherwise we do not have space for computing the precision for (\ref{eq:deltaminusmax}).

In case the conditional statement is verified,
the precision tuning phase starts.
For the precision tuning of activation functions, we allow an error that is bounded by \texttt{max\_err}. This is exactly what is described in (\ref{eq:deltaminusmax}).

On the other hand, the quantization of polytopes borders (the region bounds in \statespace) can produce an error that is at most $\varepsilon_{Q}$, in this way we satisfy 
(\ref{eq:quantizationlines}).

In case the conditional fails and \texttt{maxUij} is greater than (or equal to) delta, the controller needs to be re-designed with a robustness value that is at least equal to the current value of \texttt{maxUij}. The same consideration done for the initialization of input parameters holds also here: the constant $\varepsilon_{SAFE}$ is used to relax the value for delta, to a value slightly greater than \texttt{maxUij}. In this way, we aim to give some space to the precision tuning phase in the next iteration of the loop. Otherwise, in case $\varepsilon_{SAFE}$ is equal to zero, the next iteration of the precision tuning phase is going to require unnecessarily wide precision value for p, usually greater than 32 bits. 
We remark that the point of the analysis is to find a low precision configuration for bounds and activation functions: in case the analysis outputs a precision greater than 32bits, we fail in our goal.
Then, with a minimal alteration to the upper bound of delta, we sensibly reduce the precision needed for $F$ and $G$, with respect to the standard 32 bits precision.


