%!TEX root = main.tex
\subsection{Finite-Precision Implementation}

We now discuss finite-precision issues in the implementation of a controller.
% The controller has to be stored and evaluated on a microcontroller in finite precision.
%  since infinite-precision real values cannot be stored on today's digital computers.
The standard choice for low-power platforms is fixed-point arithmetic. Unlike
floating-point arithmetic~\cite{ieee75408}, fixed-point arithmetic can be implemented efficiently
without complex hardware support using only integer operations. It requires,
however, more compilation effort to determine certain operations statically at
compile time that the floating-point hardware unit performs dynamically.
 
% general blob about fixed-point arithmetic
The main task at compile time is to select the fixed-point format for each input and intermediate value
in the program. The format specifies the total word length and how many bits are
available for the integer part of the number. Given ranges on program inputs, a
range analysis is usually used to estimate ranges of all intermediate values in the
program~\cite{Pang2011,Kinsman2009}, which in turn determine how many integer bits are sufficient to
avoid overflow. The remaining bits of the total word length represent the
fractional part of a number; more fractional bits provide more precision. 

% introduces roundoff errors, these can be tracked with a dataflow analysis
Finite precision introduces roundoff errors, which can accumulate
during the course of a computation.
A sound static roundoff error analysis computes a guaranteed upper bound on the
error of the result by tracking worst-case errors at every operation. Given a
word length for each value, several tools, including the tool Daisy which we use, automatically determine the number of
integer bits needed and compute roundoff errors using a dataflow analysis~\cite{Daisy,Gappa}.
They track real-valued ranges at every intermediate operation. These ranges
determine the fixed-point formats and thus also the individual worst-case roundoff errors,
which the analyses track separately from the ranges. To ensure guaranteed
error bounds, both the ranges and errors are computed using interval~\cite{intervals} or
affine arithmetic~\cite{affineArithmetic}, which compute sound enclosures. 
Affine arithmetic tracks linear correlations between variables, so for linear expressions
the computed ranges are as tight as possible (nonlinear arithmetic
leads to over-approximations).
% A standard technique to increase accuracy of the computed errors is interval
% subdivision, which divides the input domain into equally-sized subdomains and
% runs roundoff error analysis separately for each subdomain. The
% over-approximations on each subdomain tend to be smaller, which increases the
% overall tightness of range and error bounds.

% mixed-precision tuning
To reduce memory usage and increase efficiency, we want to choose as short word
lengths as possible. This will minimize the memory footprint without influencing the run-time. Mixed-precision tuning tools automatically determine possibly different
word lengths for different values~\cite{DaisyTuning,Lee2006}. Compared to uniform precision (or word length),
mixed-precision often leads to improved resource usage, but due to the complexity of
fixed-point arithmetic and the error analysis, is challenging to do manually.  
Automated mixed-precision tuning tools, including Daisy, usually perform a search: they repeatedly
select a candidate mixed-precision assignment (i.e. different word lengths for
different values) and check whether the assignment satisfies a given error
bound. The candidate assignments are chosen based on a heuristic which guides
the search towards promising candidates, e.g. using delta-debugging~\cite{DaisyTuning} or
simulated annealing~\cite{Lee2006}.

In this work, we use the open-source tool Daisy which implements roundoff error
analysis~\cite{Daisy} and mixed-precision tuning~\cite{DaisyTuning} for fixed-point arithmetic.
% and is available as open-source. 
Fixed-point arithmetic implementations can choose different
rounding modes; here we consider truncation, which is more efficient than rounding.
