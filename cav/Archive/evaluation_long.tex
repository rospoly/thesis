\section{Experimental Evaluation}
\label{sec:evaluation}


We evaluate \Tool on the standard FPBench
benchmark suite~\cite{fpbench,fpbench-web} that uses the four basic operations we
currently support $\{+,-,\times,\div\}$.
%
Many of these benchmarks were also used in recent related work~\cite{probdaisy}
that we compare against.
%
The benchmarks come from a variety of domains: embedded software (\emph{bsplines}), linear classifications (\emph{classids}), physics computations (\emph{dopplers}), filters (\emph{filters}), controllers (\emph{traincars}, \emph{rigidBody}), polynomial approximations of functions (\emph{sine}, \emph{sqrt}), solving equations (\emph{solvecubic}), and global optimizations (\emph{trids}).
%
Since FPBench has been primarily used for worst-case roundoff error
analysis,  the benchmarks come with ranges for input variables, but they do not specify
input distributions.
%
We instantiate the benchmarks with three well-known distributions for all the
inputs: uniform, standard normal distribution,  and double exponential (\ie Laplace)
distribution with $\sigma=0.01$ which we will call `exp'. 
%
The normal and exp distributions get truncated to the given range.
%
We assume single-precision floating-point format for all
operands and operations, but it is straightforward to
extend \Tool to mixed-precision computations.


To assess the accuracy and performance of \Tool, we compare it with
PrAn~\cite{probdaisy}, the current state-of-the-art tool for automated analysis
of probabilistic roundoff errors.
%
PrAn currently supports only uniform and normal input distributions.
%
It offers six different tool configurations; for each benchmark, we run
all of them and report the best result.
%
We fix the number of intervals in each discretization to 50 to match PrAn.
%
We choose 99\% as the confidence interval for the computation of our conditional roundoff error (\cref{subsec:conderror}) and of PrAn's probabilistic error.
%d
We also compare our probabilistic error bounds against FPTaylor which performs worst-case roundoff error analysis, and hence it does not
take into account the distributions of the input variables.
%
We ran our experiments in parallel on a 4-socket 2.2 GHz 8-core Intel Xeon
E5-4620 machine with 128 GB of memory.

\input{tables/table-err-ranges}


Table~\ref{erroranalysis} compares roundoff errors reported by \Tool, PrAn, and
FPTaylor.
%
\Tool outperforms PrAn by computing tighter probabilistic error bounds on
almost all benchmarks, occasionally by orders of magnitude.
%
In the case of uniform input distributions, \Tool provides tighter bounds for
20 out of 27 benchmarks, for 6 benchmarks the bounds from PrAn are tighter,
while for \emph{sqrt} they are the same.
%
In the case of normal input distributions, \Tool provides tighter bounds for 26
out of 27 benchmarks, while for \emph{bspline3} the bounds from PrAn are
tighter.
%
%We provide a likely explanation for this in \cref{sec:related-work}.
%
Unlike PrAn, \Tool supports probabilistic output range analysis as well.
%
Table~\ref{rangeanalysis} in \cref{sec:appdx-evaluation} presents detailed
range analysis results.
%, which we do not discuss in detail due to lack of space.


In Table~\ref{erroranalysis}, of particular interest are benchmarks (6 for
normal and 10 for exp) where the error bounds generated by \Tool for the 99\%
confidence interval are at least an order of magnitude tighter than the
worst-case bounds generated by FPTaylor.
%
For such a benchmark and input distribution, \Tool's results inform a user that
there is an opportunity to optimize the benchmark (e.g., by reducing precision
of floating-point operations) if their use-case can handle at most 1\% of
inputs generating roundoff errors that exceed a user-provided bound.
%
FPTaylor's results, on the other hand, do not allow for a user to explore such
fine-grained trade-offs since they are worst-case and do not take probabilities
into account.

\begin{figure}[tb]
	\centering
	\begin{tabular}{l l}
		\includegraphics[width=0.5\textwidth]{pics/traincars3_range_CDF.png}
		&
		\includegraphics[width=0.5\textwidth]{pics/traincars3_abs_error_CDF.png}
		\\
		\includegraphics[width=0.5\textwidth]{pics/traincars3_gaussian_range_CDF.png}
		&
		\includegraphics[width=0.5\textwidth]{pics/traincars3_gaussian_abs_error_CDF.png}
		\\
		\includegraphics[width=0.5\textwidth]{pics/traincars3_exp_range_CDF.png}
		&
		\includegraphics[width=0.5\textwidth]{pics/traincars3_exp_abs_error_CDF.png}
	\end{tabular}
	\caption{CDFs of the range (left) and error (right) distributions for the benchmark \emph{train3} for uniform (top), normal (center), and exp (bottom).}
	\label{fig:range_error_traincar}
\end{figure}


In general, we see a gradual reduction of the errors transitioning from uniform to normal to exp.
%
When the input distributions are uniform, there is a significant chance of generating a roundoff error of the same order of magnitude as the worst-case error, because all the inputs, including the extrema, are equally likely.
%
The standard normal distribution concentrates more than 99\% of probability mass in the interval $[-3, 3]$, thus resulting in the \emph{long tail} phenomenon, where less than 0.5\% of mass spreads in the interval $[3, \infty]$.\todo[inline]{Fred: is the input range is [20,20000] do you take the standard normal centred around the mid point? the leftmost point? around zero? Same question for exp.}
%
When the normal distribution gets truncated in a neighborhood of zero (e.g., $[0,1]$ for \emph{bsplines} and \emph{filters}) nothing changes with respect to the uniform case --- there is still a high chance of committing errors close to the worst-case.
%
However, when the normal distribution gets truncated in a wider range (e.g., $[-100, 100]$ for \emph{trids}) and we do have long tails, then the outliers causing large errors are very rare events, not included in the 99\% confidence interval.
%
The exponential distribution further compresses the 99\% probability mass in the tiny interval $[-0.01, 0.01]$, so the long tails effect is common among all the benchmarks. 
%
%Indeed, it suffices to have the interval $[-1, 1]$ to have already the \emph{long tails} effect.
%
%The only exception is \emph{bspline0}.
%
%For this benchmark, at \emph{any} intermediate computation, the probability mass associated to the extrema of the distribution is always significant. 
%
%There is then an hight chance of committing rounding errors close to the worst-case for all three input distributions.
%
%Please note for all the benchmarks the 100\% error is always equal to the uniform case.
%
%This means worst-case errors are not rare events for all three input distributions.
%
%Similar argument holds for \emph{bplines}, \emph{filters}, and \emph{sqrt} benchmarks in case of uniform and normal distributions. 

%The input distributions are truncated in a neighbor of zero (e.g. $[0, 1]$ for \emph{bsplines}, and $[-2, 2]$ for \emph{filters}). Near zero, the extrema of a normal distribution are still significant. In other words, sampling close to $1$, $-2$ or $2$ in case of normal input is not a rare event.
%

%When the input is exponentially distributed, the error bounds are further reduces compared to the normal case.
%
%The exponential distribution shrinks most of the probability mass (more than 99\%) in a tiny neighbor around zero. Indeed, between $-0.01$ and $0.01$ there is more than 99\% of the distribution.
%
%The error bounds from \Tool, PrAn, and FPTaylor are almost always in the same order of magnitude.
%
The runtimes of \Tool vary between 10 minutes for small benchmarks, such as
\emph{bsplines}, to several hours for benchmarks with more than 30 operations,
such as \emph{trid4}; they are always less than two hours, except for
\emph{trids} with 11 hours and \emph{filters} with 6 hours.
%
The runtime of \Tool is usually dominated by Z3 invocations, and the long
runtimes are caused by numerous Z3 timeouts that the respective benchmarks
induce.
%
The runtimes of PrAn are comparable to \Tool since they are always less than
two hours, except for \emph{trids} with 3 hours, \emph{sqrt} with 3 hours, and
\emph{sine} with 11 hours.
%
Note that neither \Tool nor PrAn are memory intensive, and hence memory
consumption is not an issue.



To assess the quality of our rigorous (i.e., sound) results, we implement
Monte Carlo sampling to generate both roundoff error and output range
distributions.
%
The procedure consists of randomly sampling from the provided input
distributions, evaluating the floating-point computation in both the specified
and high-precision (e.g., double-precision) floating-point regimes to measure
the roundoff error, and finally partitioning the computed errors into bins to
get an approximation (i.e., histogram) of the PDF.
%
Of course, Monte Carlo sampling does not provide rigorous bounds, but is a
useful tool to assess how far the rigorous bounds computed statically by \Tool
are from an empirical measure of the error.


Fig.\ref{fig:range_error_traincar} shows the effects of the input distributions
on the output and roundoff error ranges of the \emph{traincars3} benchmark.
%
In the error graphs (right column), we show the Monte Carlo sampling evaluation
(yellow line) together with the error bounds from \Tool with 99\% confidence
interval (red plus symbol) and FPTaylor's worst-case bounds (green crossmark). 
%
In the range graphs (left column), we also plot \Tool's p-box
over-approximations.
%
We can observe that in the case of uniform inputs the computed p-boxes overlap
at the extrema of the output range.
%
This phenomenon makes it impossible to distinguish between 99\% and 100\%
confidence intervals, and hence as expected the bound reported by \Tool is
almost identical to FPTaylor's.
%
This is not the case for normal and exponential distributions, where we can
observe the long tail phenomenon, and \Tool can significantly improve both the
output range and error bounds over FPTaylor.
%
%Also, we can see how the bounds from FPTaylor, both for the range and for the error, are always identical no matter the input distribution.
%
Hence, we again illustrate how pessimistic the bounds from worst-case tools can be when the information about the input distributions
is not taken into account.
%
%The empirical error has the shape of a curve in the uniform case, but becomes similar to a straight line for the exponential distribution. 
%
Finally, the graphs illustrate how the rigorous p-boxes and error bounds from
\Tool follow their respective empirical estimations, showing that \Tool adjusts
them based on the shape of the input distribution.

